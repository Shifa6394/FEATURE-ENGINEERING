{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMXRD6zJerf5uvF9RB45Pmx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2-LjHMJjyQi6"},"outputs":[],"source":["### Q.1) What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."]},{"cell_type":"code","source":["ans)  Min-Max scaling transforms numerical data to a fixed range (typically 0 to 1) using the formula:\n","X_scaled = (X - X_min) / (X_max - X_min)\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Create sample data\n","data = pd.DataFrame({\n","    'salary': [45000, 65000, 95000, 125000, 85000],\n","    'age': [24, 35, 45, 52, 38]\n","})\n","\n","# Define MinMax scaler\n","def min_max_scale(x):\n","    return (x - x.min()) / (x.max() - x.min())\n","\n","# Apply scaling\n","scaled_data = data.apply(min_max_scale)\n","\n","# Compare original vs scaled data\n","print(\"Original Data:\\n\", data)\n","print(\"\\nScaled Data:\\n\", scaled_data)\n","\n","# Visualization\n","plt.figure(figsize=(10, 4))\n","\n","plt.subplot(1, 2, 1)\n","plt.scatter(data['age'], data['salary'])\n","plt.title('Original Data')\n","plt.xlabel('Age')\n","plt.ylabel('Salary')\n","\n","plt.subplot(1, 2, 2)\n","plt.scatter(scaled_data['age'], scaled_data['salary'])\n","plt.title('Scaled Data')\n","plt.xlabel('Age (Scaled)')\n","plt.ylabel('Salary (Scaled)')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","\n","Common use cases include:\n","\n","1. Neural network inputs where features need comparable scales\n","2. Distance-based algorithms like k-nearest neighbors\n","3. Gradient descent optimization\n","4. Feature visualization and comparison"],"metadata":{"id":"67_FTZnly4H-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.2) What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."],"metadata":{"id":"5VOVspAIyRTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans)   Unit Vector scaling (or normalization) transforms features to have a unit norm (magnitude = 1) while preserving direction, using the formula:\n","\n","X_normalized = X / ||X|| where ||X|| is the vector norm\n","\n","Key differences from Min-Max scaling:\n","- Unit Vector maintains relative proportions between features\n","- Results always have magnitude of 1\n","- Less sensitive to outliers than Min-Max\n","\n","\n","\n","```python\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","# Sample data\n","data = np.array([[4, 2, 8],\n","                 [1, 5, 3],\n","                 [2, 2, 6]])\n","\n","# Unit Vector scaling\n","def unit_vector_scale(X):\n","    return X / np.linalg.norm(X, axis=1)[:, np.newaxis]\n","\n","# Min-Max scaling\n","def min_max_scale(X):\n","    return (X - X.min(axis=1)[:, np.newaxis]) / (X.max(axis=1) - X.min(axis=1))[:, np.newaxis]\n","\n","# Apply both scaling methods\n","unit_vector_scaled = unit_vector_scale(data)\n","min_max_scaled = min_max_scale(data)\n","\n","# Print results\n","print(\"Original Data:\\n\", data)\n","print(\"\\nUnit Vector Scaled:\\n\", unit_vector_scaled)\n","print(\"\\nMin-Max Scaled:\\n\", min_max_scaled)\n","\n","# Verify unit vectors have length 1\n","norms = np.linalg.norm(unit_vector_scaled, axis=1)\n","print(\"\\nUnit Vector Norms:\", norms)\n","\n","# Visualization\n","plt.figure(figsize=(15, 5))\n","\n","plt.subplot(131)\n","plt.scatter(data[:, 0], data[:, 1])\n","plt.title('Original Data')\n","plt.grid(True)\n","\n","plt.subplot(132)\n","plt.scatter(unit_vector_scaled[:, 0], unit_vector_scaled[:, 1])\n","plt.title('Unit Vector Scaled')\n","plt.grid(True)\n","\n","plt.subplot(133)\n","plt.scatter(min_max_scaled[:, 0], min_max_scaled[:, 1])\n","plt.title('Min-Max Scaled')\n","plt.grid(True)\n","\n","plt.tight_layout()\n","\n","```\n","\n","Common applications include:\n","- Text classification (normalizing document vectors)\n","- Image processing\n","- Neural network inputs\n","- Cosine similarity calculations"],"metadata":{"id":"68ZqcaZp0EgO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.3)  What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."],"metadata":{"id":"J_miomTwyRaW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) PCA is a statistical technique used to reduce the dimensionality of a dataset while preserving as much variance as possible. It works by identifying the directions (principal components) along which the data varies the most and projecting the data onto these components.\n","\n","Steps of PCA:\n","\n","Standardize the dataset.\n","Compute the covariance matrix.\n","Calculate the eigenvectors and eigenvalues of the covariance matrix.\n","Select the top\n","ùëò\n","k eigenvectors corresponding to the largest eigenvalues.\n","Project the data onto the new\n","ùëò\n","k-dimensional space.\n","Example:\n","Consider a dataset with two features (X1, X2). After applying PCA, we find the principal components:\n","\n","PC1 explains 70% of the variance.\n","PC2 explains 30% of the variance.\n","By keeping only PC1, we reduce the dataset to one dimension while retaining most of its variance.\n","\n"],"metadata":{"id":"y1aVSrZRyRd3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.4) What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."],"metadata":{"id":"7Mh-8S-kyRhQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) PCA and feature extraction are closely related as PCA creates new features (principal components) by combining the original features. These components capture the maximum variance in the data, effectively representing the most important aspects of the dataset.\n","\n","Using PCA for Feature Extraction:\n","\n","PCA transforms the original features into a new set of features (principal components).\n","These components are linear combinations of the original features but are uncorrelated and ranked by the amount of variance they explain.\n","Example:\n","A dataset with 5 features is reduced to 2 principal components using PCA:\n","\n","Original features: [X1, X2, X3, X4, X5].\n","Principal components: [PC1, PC2].\n","Here, PC1 and PC2 represent the most significant patterns in the data.\n","\n"],"metadata":{"id":"T9O6ol3eyRku"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.5) You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."],"metadata":{"id":"mbskQSfmyRoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) ```python\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler\n","import matplotlib.pyplot as plt\n","\n","# Sample food delivery data\n","data = pd.DataFrame({\n","    'price': [15.99, 25.50, 8.99, 45.00, 12.99],\n","    'rating': [4.5, 3.8, 4.2, 4.8, 3.5],\n","    'delivery_time': [25, 45, 15, 35, 20]\n","})\n","\n","# Initialize and apply MinMaxScaler\n","scaler = MinMaxScaler()\n","scaled_features = scaler.fit_transform(data)\n","scaled_df = pd.DataFrame(scaled_features, columns=data.columns)\n","\n","# Compare original vs scaled data\n","print(\"Original Data:\\n\", data)\n","print(\"\\nScaled Data:\\n\", scaled_df)\n","\n","# Visualize distribution before and after scaling\n","fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n","fig.suptitle('Feature Distributions Before and After Scaling')\n","\n","for i, feature in enumerate(data.columns):\n","    # Original distribution\n","    axes[0, i].hist(data[feature])\n","    axes[0, i].set_title(f'Original {feature}')\n","\n","    # Scaled distribution\n","    axes[1, i].hist(scaled_df[feature])\n","    axes[1, i].set_title(f'Scaled {feature}')\n","\n","plt.tight_layout()\n","\n","```\n","\n","Implementation steps:\n","1. Scale price to [0,1]: high prices won't dominate recommendations\n","2. Transform ratings (usually 1-5) to [0,1]: maintains relative quality differences\n","3. Scale delivery times to [0,1]: longer times get lower normalized values\n","4. Handle any missing values before scaling\n","\n","Benefits:\n","- All features contribute equally to recommendations\n","- Prevents price or delivery time from overwhelming rating importance\n","- Enables fair comparison between different restaurants/items"],"metadata":{"id":"Rvw_KdBR0osV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.6) You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n","\n"],"metadata":{"id":"b125FJ0tyRvN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) ```python\n","import pandas as pd\n","import numpy as np\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","import matplotlib.pyplot as plt\n","\n","# Generate sample stock market data\n","np.random.seed(42)\n","n_samples = 1000\n","data = pd.DataFrame({\n","    'price': np.random.normal(100, 10, n_samples),\n","    'volume': np.random.normal(1000000, 100000, n_samples),\n","    'pe_ratio': np.random.normal(15, 3, n_samples),\n","    'market_cap': np.random.normal(1000000000, 100000000, n_samples),\n","    'debt_ratio': np.random.normal(0.3, 0.1, n_samples),\n","    'revenue_growth': np.random.normal(0.1, 0.02, n_samples),\n","    'profit_margin': np.random.normal(0.15, 0.03, n_samples),\n","    'beta': np.random.normal(1, 0.2, n_samples)\n","})\n","\n","# Standardize features\n","scaler = StandardScaler()\n","scaled_data = scaler.fit_transform(data)\n","\n","# Apply PCA\n","pca = PCA()\n","pca_data = pca.fit_transform(scaled_data)\n","\n","# Plot explained variance ratio\n","plt.figure(figsize=(10, 5))\n","plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n","         np.cumsum(pca.explained_variance_ratio_), 'bo-')\n","plt.axhline(y=0.95, color='r', linestyle='--')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance Ratio')\n","plt.title('Explained Variance vs Components')\n","plt.grid(True)\n","\n","# Print component contributions\n","print(\"Explained variance ratio:\", pca.explained_variance_ratio_)\n","print(\"\\nFeature loadings (first 2 components):\")\n","loadings = pd.DataFrame(\n","    pca.components_.T,\n","    columns=[f'PC{i+1}' for i in range(len(data.columns))],\n","    index=data.columns\n",")\n","print(loadings[['PC1', 'PC2']])\n","\n","```\n","\n","Implementation steps:\n","1. Standardize features (crucial for financial data with different scales)\n","2. Apply PCA, examine explained variance\n","3. Select components explaining 95% variance\n","4. Transform data using selected components\n","5. Use transformed features for prediction model\n","\n","Key financial feature correlations typically reduced:\n","- Market cap/volume\n","- PE ratio/profit margin\n","- Revenue growth/debt ratio"],"metadata":{"id":"S7Qbl3GIyRzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.7)  For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."],"metadata":{"id":"S0Pq7WQhyR3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) ```python\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Original data\n","data = np.array([1, 5, 10, 15, 20])\n","\n","# Two-step scaling process\n","def scale_to_range(x, target_min=-1, target_max=1):\n","    # First scale to [0,1]\n","    x_std = (x - x.min()) / (x.max() - x.min())\n","    # Then scale to target range\n","    x_scaled = x_std * (target_max - target_min) + target_min\n","    return x_scaled\n","\n","# Apply scaling\n","scaled_data = scale_to_range(data)\n","\n","print(\"Original data:\", data)\n","print(\"Scaled data:\", scaled_data)\n","\n","# Verify range\n","print(\"Min:\", scaled_data.min())  # Should be -1\n","print(\"Max:\", scaled_data.max())  # Should be 1\n","\n","# Visualization\n","plt.figure(figsize=(10, 4))\n","plt.subplot(121)\n","plt.scatter(range(len(data)), data)\n","plt.title('Original Data')\n","plt.grid(True)\n","\n","plt.subplot(122)\n","plt.scatter(range(len(scaled_data)), scaled_data)\n","plt.title('Scaled Data [-1,1]')\n","plt.grid(True)\n","plt.axhline(y=0, color='r', linestyle='--')\n","plt.tight_layout()\n","\n","```\n","\n","Result: [-1.0, -0.47368421, 0.05263158, 0.57894737, 1.0]"],"metadata":{"id":"o_eY8ED8yR7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### Q.8) or a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"],"metadata":{"id":"3ncigQrCyR_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ans) The number of principal components to retain depends on the explained variance threshold. A common choice is to retain components that explain at least 95% of the total variance.\n","\n","Steps:\n","\n","Standardize the dataset.\n","Apply PCA and compute the explained variance ratio for each component.\n","Choose the smallest number of components such that the cumulative explained variance ‚â• 95%.\n","Example:\n","If the explained variance ratio for the components is:\n","\n","PC1: 50%, PC2: 30%, PC3: 15%, PC4: 5%, PC5: 0%.\n","Retain the first 3 components (50% + 30% + 15% = 95%)."],"metadata":{"id":"87sKaMebySEO"},"execution_count":null,"outputs":[]}]}